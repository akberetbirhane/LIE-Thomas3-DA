{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition (N.E.R.)\n",
    "\n",
    "Named entity recognition is a NLP technique to extract specific entities from a document.\n",
    "\n",
    "You could, for example, extract all the addresses or all the prices or both at the same time! It should be easy to do for the model as the format will almost always be the same (there are exceptions of course).\n",
    "    * All the addresses are going to contain a street name, a number, a zip code, a city and a country.\n",
    "    * All the prices are going to be number with a money symbol before or after it.\n",
    "    * All the names are going to start with an uppercase.\n",
    "\n",
    "If you are in this case, even if putting AI in the title of you project is cool, you should ask yourself if a regular expression (RegEx) is not easier and better. \n",
    "\n",
    "But you could also want to only extract prices **without** taxes, or only the address of the receiver in letters. In this case, you want a specific entity, and it's way harder to do with RegEx. That's where NER is useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The easy way: SpaCy\n",
    "\n",
    "[SpaCy](https://spacy.io/) gives you a set of pre-trained models in several languages. However, they are very general models, so general that if you have a use-case a little bit specific, it will probably not be enough. **But** if you want to do a demo or do a proof of concept, it will be one of the best way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">BeCode’s mission : Enabling \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    tomorrow\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "’s digital talents to blossom.We believe that education makes anything possible.Since \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2017\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BeCode\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " has been offering inclusive coding bootcamps for jobseekers to become developers in partnership with \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Simplon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the needed dependencies from SpaCy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "\n",
    "# Create an instance of the small pipeline and model from SpaCy\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# The text we want to process\n",
    "text = \"BeCode’s mission : Enabling tomorrow’s digital talents to blossom.We believe that education makes anything possible.Since 2017, BeCode has been offering inclusive coding bootcamps for jobseekers to become developers in partnership with Simplon.\"\n",
    "\n",
    "# Apply the complete NER pipeline from SpaCy (preprocessing + model prediction)\n",
    "doc = nlp(text)\n",
    "\n",
    "# Show the entities detected in a nice way\n",
    "displacy.render(doc, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get this information in a more \"useful\" way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomorrow: DATE\n",
      "2017: DATE\n",
      "BeCode: ORG\n",
      "Simplon: PRODUCT\n"
     ]
    }
   ],
   "source": [
    "# Loop over all the tokens preprocessed by SpaCy\n",
    "for token in doc:\n",
    "    # If the token is detected as an entity\n",
    "    if token.ent_type_:\n",
    "        print(f\"{token.text}: {token.ent_type_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy's magic\n",
    "\n",
    "Simple right? Well, that's the whole purpose of SpaCy. They provide state of the art models with simple usage.\n",
    "\n",
    "Also, the documentation of SpaCy is amazing!\n",
    "\n",
    "## Limitations\n",
    "\n",
    "If you need to train a specific model because the general one's from SpaCy are not enough for your use-case, you will be stuck quite quickly with SpaCy.\n",
    "Why? Because even if you can train a custom model with SpaCy, you will find that SpaCy is not that flexible and that's because it's not SpaCy's main purpose.\n",
    "\n",
    "## Alternatives\n",
    "\n",
    "SpaCy's equivalent with more flexibility is [NLTK](https://www.nltk.org/).\n",
    "NLTK is made as a framework to train deep learning NLP models.\n",
    "\n",
    "[BERT](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270) is one of the state of the art model architecture for NLP. But it's a bit more complex.\n",
    "BERT is a [transformer](http://jalammar.github.io/illustrated-transformer/) (yes, like the robots) but we will see that later.\n",
    "\n",
    "\n",
    "## Practice time!\n",
    "With the text:\n",
    "```\n",
    "BeCode’s mission : Enabling tomorrow's digital talents to blossom.\n",
    "We believe that education makes anything possible.\n",
    "Since 2017, BeCode has been offering inclusive coding bootcamps for job seekers to become developers in partnership with Simplon.\n",
    "```\n",
    "\n",
    "Try to extracts entities with NLTK.\n",
    "\n",
    "*You don't need to get an amazing accuracy or to get all the entities, the purpose of this exercise is just to make you understand how the library works.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\becode/nltk_data'\n    - 'c:\\\\Users\\\\becode\\\\Desktop\\\\DA\\\\LIE-Thomas3-DA\\\\.nlpvenv\\\\nltk_data'\n    - 'c:\\\\Users\\\\becode\\\\Desktop\\\\DA\\\\LIE-Thomas3-DA\\\\.nlpvenv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\becode\\\\Desktop\\\\DA\\\\LIE-Thomas3-DA\\\\.nlpvenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\becode\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Tag the tokens\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m tagged \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Extract entities\u001b[39;00m\n\u001b[0;32m     11\u001b[0m entities \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mne_chunk(tagged)\n",
      "File \u001b[1;32mc:\\Users\\becode\\Desktop\\DA\\LIE-Thomas3-DA\\.nlpvenv\\Lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\becode\\Desktop\\DA\\LIE-Thomas3-DA\\.nlpvenv\\Lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\becode\\Desktop\\DA\\LIE-Thomas3-DA\\.nlpvenv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPICKLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32mc:\\Users\\becode\\Desktop\\DA\\LIE-Thomas3-DA\\.nlpvenv\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\becode/nltk_data'\n    - 'c:\\\\Users\\\\becode\\\\Desktop\\\\DA\\\\LIE-Thomas3-DA\\\\.nlpvenv\\\\nltk_data'\n    - 'c:\\\\Users\\\\becode\\\\Desktop\\\\DA\\\\LIE-Thomas3-DA\\\\.nlpvenv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\becode\\\\Desktop\\\\DA\\\\LIE-Thomas3-DA\\\\.nlpvenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\becode\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Import NLTK\n",
    "import nltk\n",
    "# Preprocess the text\n",
    "text = \"BeCode’s mission: Enabling tomorrow’s digital talents to blossom.We believe that education makes anything possible. Since 2017, BeCode has been offering inclusive coding bootcamps for jobseekers to become developers in partnership with Simplon.\"\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "# Tag the tokens\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "# Extract entities\n",
    "entities = nltk.ne_chunk(tagged)\n",
    "\n",
    "# Print entities detected\n",
    "print(entities)\n",
    "# Or\n",
    "for tree in entities:\n",
    "    if hasattr(tree, 'label'):\n",
    "        print(tree.label(), ' '.join(c[0] for c in tree.leaves()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "* [NER with SpaCy and NTLK](https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da)\n",
    "* [What is named entity recognition (NER) and how can I use it?](https://medium.com/mysuperai/what-is-named-entity-recognition-ner-and-how-can-i-use-it-2b68cf6f545d)\n",
    "* [How does Named Entity Recognition help on Information Extraction in NLP?](https://towardsdatascience.com/named-entity-recognition-3fad3f53c91e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final word about SpaCy and NLTK\n",
    "SpaCy and NLTK are fantastic tools that you **need to have** under your belt.\n",
    "\n",
    "If you plan to work in the NLP field, all the time spent learning how to use those libraries is precious.\n",
    "\n",
    "![cool machine](https://media.giphy.com/media/1flAwtHCYosL6LWnHr/giphy.gif)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
