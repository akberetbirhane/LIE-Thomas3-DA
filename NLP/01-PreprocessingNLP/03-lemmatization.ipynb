{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "As we saw in the previous chapter, we can explain to the machine which words are similar but also how different there are.\n",
    "\n",
    "However some \"different\" words are only variations of the same word and should not be considered as different entries. \n",
    "\n",
    "Let's take an example:\n",
    "\n",
    "Imagine that you are asked to build a model to classify books in two categories: _cooking_ and _cars_. You will use the most frequent words of the book to build your algorithm.\n",
    "\n",
    "In that case you don't really want to make a distinction between `apple` and `apples` or between `wheel` and `wheels`. You prefer to consider `apple` and `apples` as being variations of `apple`.\n",
    "\n",
    "To fix that, we will apply **lemmatization**. This approach aims to reduce each word to its simplest variation (named **lemma**). This lemma corresponds to the heading word in a language dictionary:\n",
    "\n",
    "\n",
    "**apple** (noun) : `a round fruit (usually with a green or red skin) which can be eaten (plural: apples)`\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## Still confused?\n",
    "Let's see how it works in a practical case.\n",
    "\n",
    "First, read [this article](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/).\n",
    "\n",
    "Then, try to apply what you have learned by using SpaCy or NLTK.\n",
    "\n",
    "**Pro tips:** Most lemmatizers only work with a single word and not on sentences. Think about tokenizing your sentence first.\n",
    "\n",
    "**Pro tips:** If you experience SSL issues during `nltk` import [check this](https://stackoverflow.com/questions/38916452/nltk-download-ssl-certificate-verify-failed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy lemmas: ['those', 'child', 'be', 'play', '.', 'this', 'game', ',', 'those', 'game', ',', 'I', 'play', 'he', 'play']\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\becode/nltk_data'\n    - 'c:\\\\Users\\\\becode\\\\Desktop\\\\DA\\\\LIE-Thomas3-DA\\\\.nlpvenv\\\\nltk_data'\n    - 'c:\\\\Users\\\\becode\\\\Desktop\\\\DA\\\\LIE-Thomas3-DA\\\\.nlpvenv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\becode\\\\Desktop\\\\DA\\\\LIE-Thomas3-DA\\\\.nlpvenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\becode\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Lemmatize with NLTK\u001b[39;00m\n\u001b[0;32m     21\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m---> 22\u001b[0m nltk_lemmas \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word, pos) \u001b[38;5;28;01mfor\u001b[39;00m word, pos \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_sentence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNLTK lemmas:\u001b[39m\u001b[38;5;124m\"\u001b[39m, nltk_lemmas)\n",
      "File \u001b[1;32mc:\\Users\\becode\\Desktop\\DA\\LIE-Thomas3-DA\\.nlpvenv\\Lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\becode\\Desktop\\DA\\LIE-Thomas3-DA\\.nlpvenv\\Lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\becode\\Desktop\\DA\\LIE-Thomas3-DA\\.nlpvenv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPICKLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32mc:\\Users\\becode\\Desktop\\DA\\LIE-Thomas3-DA\\.nlpvenv\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\becode/nltk_data'\n    - 'c:\\\\Users\\\\becode\\\\Desktop\\\\DA\\\\LIE-Thomas3-DA\\\\.nlpvenv\\\\nltk_data'\n    - 'c:\\\\Users\\\\becode\\\\Desktop\\\\DA\\\\LIE-Thomas3-DA\\\\.nlpvenv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\becode\\\\Desktop\\\\DA\\\\LIE-Thomas3-DA\\\\.nlpvenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\becode\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Can you lemmatize this sentence with Spacy and / or NLTK?\n",
    "\n",
    "my_sentence = \"Those children are playing. this game, those games, I play he plays\"\n",
    "# Import the libraries\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load the Spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the sentence\n",
    "my_sentence = \"Those children are playing. this game, those games, I play he plays\"\n",
    "\n",
    "# Lemmatize with Spacy\n",
    "doc = nlp(my_sentence)\n",
    "spacy_lemmas = [token.lemma_ for token in doc]\n",
    "print(\"Spacy lemmas:\", spacy_lemmas)\n",
    "\n",
    "# Lemmatize with NLTK\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk_lemmas = [lemmatizer.lemmatize(word, pos) for word, pos in nltk.pos_tag(nltk.word_tokenize(my_sentence))]\n",
    "print(\"NLTK lemmas:\", nltk_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the differences between both tools ?\n",
    "\n",
    "## Conclusion\n",
    "There are multiple libraries that allow you to do lemmatization. Each of them have their particularities.\n",
    "There are also other techniques to \"simplify\" words like [Stemming](https://medium.com/swlh/introduction-to-stemming-vs-lemmatization-nlp-8c69eb43ecfe). Feel free explore those that seems relevant to your use-case.\n",
    "\n",
    "![stemming vs lemmatization](https://miro.medium.com/max/2050/1*ES5bt7IoInIq2YioQp2zcQ.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
